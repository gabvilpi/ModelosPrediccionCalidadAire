---
title: "Predicción de la calidad del aire de Madrid mediante modelos supervisados"
subtitle: "
Máster universitario en Ciencia de Datos (Data Science)
Minería de datos y Machine Learning"
author: "Gabriel Villalba Pintado"
date: "`r format(Sys.Date(), '%d de %B de %Y')`"
output: 
  html_document:
    toc: true
    toc_depth: 4
    theme: united
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Predicción de la calidad del aire de Madrid mediante modelos supervisados

## 1. Comprensión de los datos

### 1.1. Carga de datos

#### 1.1.1. Calidad del aire

A continuación se realiza la carga recursiva de los datos de calidad del aire en el periodo 2013-2018. Los ficheros fuente contienen los datos en texto plano con los campos delimitados por un ancho determinado. Las medidas no válidas se inican como "00.00N" y las válidas tienen una letra "V"" después de la medición. Se unificarán los campos "año", "mes" y "dia" en un único campo "fecha". Los datos consolidados se almacenarán en un fichero de texto plano delimitado por comas (CSV).

```{r carga_contaminacion, warning=FALSE, message=FALSE}
# Path datos contaminación "Datos/Contaminación"
setwd("Datos/Contaminación")

# Listamos los ficheros a cargar
ficheros <- list.files(pattern = ".*mo1[3-8].txt")


# Determinamos el ancho de cada campo
anchos <- c(8,2,2,2,2,2,2,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6)

# Determinamos el nombre de las columnas
columnas <- c("estacion","contaminante","tecnica","periodo","anyo","mes","dia","hora1","hora2","hora3","hora4","hora5","hora6","hora7","hora8","hora9","hora10","hora11","hora12","hora13","hora14","hora15","hora16","hora17","hora18","hora19","hora20","hora21","hora22","hora23","hora24")

# Cargamos los datos en un DataFrame. Las medidas no válidas ("00.00N") se sobreescriben con "NA"
for (fichero in ficheros){
  
  # Si el DataSet existe, creamos un DataSet auxiliar para unir los datos
  if (exists("calidadAire")){
    temp_dataset <- read.fwf(file = fichero, widths = anchos, header = FALSE, col.names = columnas, na.strings = c("00.00N"))
    calidadAire <- rbind(calidadAire, temp_dataset)
    rm(temp_dataset)
  }
       
  # Si el DataSet no existe, lo creamos
  if (!exists("calidadAire")){
    calidadAire <- read.fwf(file = fichero, widths = anchos, header = FALSE, col.names = columnas, na.strings = c("00.00N"))
  }
 
}

# Unimos las columnas año, mes y dia en una única columna fecha
calidadAire<-transform(calidadAire, anyo=paste(anyo, mes, dia, sep="/"))
colnames(calidadAire)[5]<-"fecha"
calidadAire<-calidadAire[,!names(calidadAire) %in% c("mes", "dia")]

# Eliminamos la V de los valores válidos de cada medición horaria
for (i in 6:29) {
  calidadAire[,i]<-gsub("V","",calidadAire[,i])
}

# Corregimos el tipo de dato de cada columna
# calidadAire<-transform(calidadAire, estacion=as.factor(estacion), contaminante=as.factor(contaminante), tecnica=as.factor(tecnica), periodo=as.factor(periodo), fecha=as.Date(fecha, format("%y/%m/%d")), hora1=as.numeric(hora1), hora2=as.numeric(hora2), hora3=as.numeric(hora3), hora4=as.numeric(hora4), hora5=as.numeric(hora5), hora6=as.numeric(hora6), hora7=as.numeric(hora7), hora8=as.numeric(hora8), hora9=as.numeric(hora9), hora10=as.numeric(hora10), hora11=as.numeric(hora11), hora12=as.numeric(hora12), hora13=as.numeric(hora13), hora14=as.numeric(hora14), hora15=as.numeric(hora15), hora16=as.numeric(hora16), hora17=as.numeric(hora17), hora18=as.numeric(hora18), hora19=as.numeric(hora19), hora20=as.numeric(hora20), hora21=as.numeric(hora21), hora22=as.numeric(hora22), hora23=as.numeric(hora23), hora24=as.numeric(hora24))

# Corregimos el tipo de dato de cada columna
calidadAire <- transform(calidadAire, estacion=as.factor(estacion), contaminante=as.factor(contaminante), tecnica=as.factor(tecnica), periodo=as.factor(periodo), fecha=as.Date(fecha, format("%y/%m/%d")))
calidadAire[,6:29] <- sapply(calidadAire[,6:29], as.numeric)

# Ordenamos el data frame por fecha, estación y contaminante
calidadAire <- calidadAire[order(calidadAire$fecha, calidadAire$estacion, calidadAire$contaminante),]

# Guardamos los datos
write.csv(calidadAire, file = "calidadAire.csv", row.names = FALSE)

```

#### 1.1.2. Meteorología

En el siguiente trozo de código se procede a realizar la carga recursiva de los ficheros de meteorología. Los ficheros contienen los datos en texto plano con los diferentes campos separados por comas (CSV). Se procederá a eliminar el número de fila y a modificar el nivel de precipitación "Ip" (inapreciable ó <0.01) por el valor 0.05. Los datos consolidados se almacenarán en un fichero CSV.

```{r carga_meteorologia, warning=FALSE, message=FALSE}
# Path datos meteorología "Datos/Clima"
setwd("Datos/Clima")

# Listamos los ficheros a cargar
ficheros <- list.files(pattern = "weather_201[3-8].*.csv")

# Determinamos el nombre de las columnas
columnas <- c("id","altitud","dir","fecha","horaPresMax","horaPresMin","horaRacha","horaTmax","horaTmin","indicativo","nombre","prec","presMax","presMin","provincia","racha","tmax","tmed","tmin","velMedia")

# Determinamos el tipo de dato de cada columna en la lectura del fichero
colClasses = c("character","character","character","Date","character","character","character","character","character","character","character","character","character","character","character","character","character","character","character","character")

# Cargamos los datos en un DataFrame
for (fichero in ficheros){
  
  # Si el DataSet existe, creamos un DataSet auxiliar para unir los datos
  if (exists("clima")){
    temp_dataset <- read.csv(file = fichero, header = TRUE, sep = ",", col.names = columnas, colClasses = colClasses, na.strings = c(""))
    clima <- rbind(clima, temp_dataset)
    rm(temp_dataset)
  }
       
  # Si el DataSet no existe, lo creamos
  if (!exists("clima")){
    clima <- read.csv(file = fichero, header = TRUE, sep = ",", col.names = columnas, colClasses = colClasses, na.strings = c(""))
  }
 
}

# Eliminamos del data frame el indicador de fila
clima <- clima[2:20]

# Sustituimos la medida de precipitación inapreciable (Ip < 0.1) por el valor de 0.05
clima$prec <- gsub("Ip", "0.05", clima$prec)

# Corregimos el tipo de dato de cada columna
clima <- transform(clima, altitud = as.integer(altitud), dir = as.integer(dir), indicativo = as.factor(indicativo), nombre = as.factor(nombre), prec = as.numeric(sub(",", ".", prec)), presMax = as.numeric(sub(",", ".", presMax)), presMin = as.numeric(sub(",", ".", presMin)), provincia = as.factor(provincia), racha = as.numeric(sub(",", ".", racha)), tmax = as.numeric(sub(",", ".", tmax)), tmed = as.numeric(sub(",", ".", tmed)), tmin = as.numeric(sub(",", ".", tmin)), velMedia = as.numeric(sub(",", ".", velMedia)))

# Guardamos los datos
write.csv(clima, file = "clima.csv", row.names = FALSE)

```

#### 1.1.3. Calendario laboral

Por último, se realiza la carga del calendario laboral. Se trata de un fichero en texto plano con los campos separados por punto y coma (CSV). Se procederá a normalizar los campos diaSemana y tipoFestivo. Los datos cargados y normalizados se almacenarán en un fichero CSV.

```{r carga_calendario, warning=FALSE, message=FALSE}
# Path datos meteorología "Datos/Clima"
setwd("Datos/Calendario")

# Listamos los ficheros a cargar
ficheros <- list.files(pattern = "Calendario.*.csv")

# Determinamos el nombre de las columnas
columnas <- c("fecha","diaSemana","tipoDia","tipoFestivo","festividad")

# Cargamos los datos en un DataFrame
for (fichero in ficheros){
  
  # Si el DataSet existe, creamos un DataSet auxiliar para unir los datos
  if (exists("calendario")){
    temp_dataset <- read.csv(file = fichero, header = TRUE, sep = ";", col.names = columnas)
    calendario <- rbind(clima, temp_dataset)
    rm(temp_dataset)
  }
       
  # Si el DataSet no existe, lo creamos
  if (!exists("calendario")){
    calendario <- read.csv(file = fichero, header = TRUE, sep = ";", col.names = columnas)
  }
 
}

# Corregimos el tipo de dato de la columna dia
calendario<-transform(calendario, fecha=as.Date(fecha, format("%d/%m/%Y")))

# Corregimos los niveles de algúnos de los factores
levels(calendario$diaSemana)<-gsub("miercoles", "miércoles", levels(calendario$diaSemana))
levels(calendario$diaSemana)<-gsub("sabado", "sábado", levels(calendario$diaSemana))
levels(calendario$tipoFestivo)<-gsub("Festivo de la comunidad de Madrid", "Festivo de la Comunidad de Madrid", levels(calendario$tipoFestivo))

# Corregimos los niveles vacíos por "laborables"
levels(calendario$tipoDia)<-c("laborable", "domingo", "festivo", "laborable", "sabado")

# Guardamos los datos
write.csv(calendario, file = "calendario.csv", row.names = FALSE)

```

### 1.2. Exploración de los datos

#### 1.2.1. Calidad del aire

```{r exploracion_calidadAire, warning=FALSE, message=FALSE}

contaminante = c("Dióxido de Azufre","Monóxido de Carbono","Monóxido de Nitrógeno","Dióxido de Nitrógeno","Partículas PM2.5","Partículas PM10","Óxidos de Nitrógeno","Ozono","Tolueno","Benceno","Etilbenceno","Hidrocarburos totales
(hexano)","Metano","Hidrocarburos
no metánicos (hexano)")

j=1

# Boxplot mediciones de calidadAire
for (i in levels(calidadAire$contaminante)){
  boxplot(calidadAire[calidadAire$contaminante==i,6:29], main=contaminante[j], outline = FALSE)
  j=j+1
}

```

#### 1.2.2. Meteorología

```{r exploracion_clima, warning=FALSE, message=FALSE}
# str()
str(clima)

# summary()
summary(clima)

# Gráficos
library(ggplot2)

# Precipitaciones
ggplot(data = clima, aes(fecha, prec)) + geom_line() + scale_x_date(date_breaks = "1 month", date_labels = "%m", limits=as.Date(c('2018-01-01','2018-12-31'))) + ggtitle("Precipitaciones en 2018 (mm)")

# Presión atmosférica
ggplot(data = clima, aes(fecha)) + geom_line(aes(y = presMax, colour = "presMax")) + geom_line(aes(y = presMin, colour = "presMin")) + scale_x_date(date_breaks = "1 month", date_labels = "%m", limits=as.Date(c('2018-01-01','2018-12-31'))) + ggtitle("Presión max y min en 2018 (hPa)")

# Temperatura
ggplot(data = clima, aes(fecha)) + geom_line(aes(y = tmax, colour = "tmax")) + geom_line(aes(y = tmed, colour = "tmed")) + geom_line(aes(y = tmin, colour = "tmin")) + scale_x_date(date_breaks = "1 month", date_labels = "%m", limits=as.Date(c('2018-01-01','2018-12-31'))) + ggtitle("Temperatura máxima, media y mínima en 2018 (ºC)")

# Viento
ggplot(data = clima, aes(fecha)) + geom_line(aes(y = racha, colour = "racha")) + geom_line(aes(y = velMedia, colour = "velMedia")) + scale_x_date(date_breaks = "1 month", date_labels = "%m", limits=as.Date(c('2018-01-01','2018-12-31'))) + ggtitle("Velocidad máxima y media del viento en 2018 (m/s)")

```

#### 1.2.3. Calendario laboral

```{r exploracion_calendario, warning=FALSE, message=FALSE}
# str()
str(calendario)

# summary()
summary(calendario)

```

### 1.3. Preparación de los datos

#### 1.3.1. Seleccionar e integrar los datos

A continuación se seleccionan los datos deseados para constuir el modelo de predicción de la calidad del aire. Los datos seleccionados se integrarán en un nuevo data frame uniendo todos los datos de una misma fecha. 

Del data frame "calidadAire" se seleccionarán los datos correspondientes a la fecha de la medida, la estación, el contaminante medido y las 24 medidas horarias. Del data frame "clima" se seleccionará la fecha y los datos de medidas de precipitación, presión máxima y mínima, temperatura máxima, mínima y media, y velocidad media y máxima del viento. Por último, del data frame "calendario" se seleccionará la fecha y si ese dia es fesitivo o no.

```{r seleccionar_integrar, warning=FALSE, message=FALSE}

# Seleccionamos los datos del data frame calidadAire y creamos un data frame nuevo
datos <- calidadAire[,c("estacion","fecha","contaminante","hora1","hora2","hora3","hora4","hora5","hora6","hora7","hora8","hora9","hora10","hora11","hora12","hora13","hora14","hora15","hora16","hora17","hora18","hora19","hora20","hora21","hora22","hora23","hora24")]

# Uniremos los data frames como data table
require(data.table)
setDT(datos)

# Unimos los datos seleccionados del data frame clima según la fecha de las mediciones
datos<-datos[clima[,c("fecha","prec","presMax","presMin","tmax","tmed","tmin","racha","velMedia")], on = "fecha"]

# Unimos los datos seleccionados del data frame calendario según la fecha de las mediciones
datos<-datos[calendario[calendario$fecha<'2019-01-01',c("fecha","tipoDia")], on = "fecha"]

# Reconvertimos la data table en data frame
datos<-as.data.frame(datos)

```

Una vez integrados los datos en el data frame "datos" se realizarán las siguientes transformaciones:

- Se sustituye el nombre del campo "festivo" por "laborable" y lo codificamos como 0|1 (festivo = 0, sabado = 0, domingo = 1, laborable = 1)

```{r codificación_laborable, warning=FALSE, message=FALSE}

# Sustituimos el nombre del campo "festivo" por "laborable"
colnames(datos)[36]<-"laborable"

# Codificamos el campo laborable
levels(datos$laborable)<-c(1,0,0,0)

# Cambiamos el tipo de dato del campo laborable a numérico
datos$laborable<-as.numeric(levels(datos$laborable)[datos$laborable])

```

- Se imputa a los valores perdidos (NA) el valor de la media aritmética del resto de valores observados

```{r imputación_valores, message=FALSE, warning=FALSE}

for (i in levels(datos$contaminante)) {
  for (j in levels(datos$estacion)) {
    for (k in colnames(datos)[4:35]) {
      temp<-is.na(datos[[k]])
      if (nrow(datos[temp & datos$contaminante==i & datos$estacion==j,][k]) != 0) {
        datos[temp & datos$contaminante==i & datos$estacion==j,][k] <- mean(data.matrix(datos[datos$contaminante==i & datos$estacion==j,][k]), na.rm = TRUE)
      }
      rm(temp)
    }
  }
}

# Guardamos los datos
write.csv(datos, file = "datos.csv", row.names = FALSE)

```

Realizamos una nueva exploración de los datos una vez imputado a los valores perdidos (NA) el valor de la media aritmética del resto de valores observados

```{r exploración_datos, message=FALSE, warning=FALSE}
j=1

# Boxplot datos contaminación
for (i in levels(datos$contaminante)){
  boxplot(datos[datos$contaminante==i,4:27], main=contaminante[j], outline = FALSE)
  j=j+1
}

# Gráficos
library(ggplot2)

# Precipitaciones
ggplot(data = datos, aes(fecha, prec)) + geom_line() + scale_x_date(date_breaks = "1 month", date_labels = "%m", limits=as.Date(c('2018-01-01','2018-12-31'))) + ggtitle("Precipitaciones en 2018 (mm)")

# Presión atmosférica
ggplot(data = datos, aes(fecha)) + geom_line(aes(y = presMax, colour = "presMax")) + geom_line(aes(y = presMin, colour = "presMin")) + scale_x_date(date_breaks = "1 month", date_labels = "%m", limits=as.Date(c('2018-01-01','2018-12-31'))) + ggtitle("Presión max y min en 2018 (hPa)")

# Temperatura
ggplot(data = datos, aes(fecha)) + geom_line(aes(y = tmax, colour = "tmax")) + geom_line(aes(y = tmed, colour = "tmed")) + geom_line(aes(y = tmin, colour = "tmin")) + scale_x_date(date_breaks = "1 month", date_labels = "%m", limits=as.Date(c('2018-01-01','2018-12-31'))) + ggtitle("Temperatura máxima, media y mínima en 2018 (ºC)")

# Viento
ggplot(data = datos, aes(fecha)) + geom_line(aes(y = racha, colour = "racha")) + geom_line(aes(y = velMedia, colour = "velMedia")) + scale_x_date(date_breaks = "1 month", date_labels = "%m", limits=as.Date(c('2018-01-01','2018-12-31'))) + ggtitle("Velocidad máxima y media del viento en 2018 (m/s)")
```

- Se incluye una columna con la media y otra con el máximo de las mediciones horarias de contaminación en esa estación para cada día

```{r media_y_max_diario_contaminación}

# Calculamos el nivel medio de contaminación en cada estación para cada día
datos <- transform(datos, media = rowMeans(data.matrix(datos[,4:27])))

# Cargamos la librería matrixStats para usar la función rowMaxs
library(matrixStats)

# Calculamos el nivel máximo de contaminación en cada estación para cada día
datos <- transform(datos, maximo = rowMaxs(data.matrix(datos[,4:27])))

# Reordenamos las columnas
datos<-datos[,c("estacion","fecha","contaminante","hora1","hora2","hora3","hora4","hora5","hora6","hora7","hora8","hora9","hora10","hora11","hora12","hora13","hora14","hora15","hora16","hora17","hora18","hora19","hora20","hora21","hora22","hora23","hora24","media","maximo","prec","presMax","presMin","tmax","tmed","tmin","racha","velMedia","laborable")]

# Guardamos los datos
write.csv(datos, file = "datos.csv", row.names = FALSE)

```

- Se normalizan todas las medidas mediante *z-score* en un nuevo dataframe "datosNorm": 
$$z_i = \frac{x_i - \overline{x}}{\sigma}$$

```{r normalización, message=FALSE, warning=FALSE}

datosNorm <- datos

for (i in levels(datosNorm$contaminante)) {
  for (j in levels(datosNorm$estacion)) {
    if (nrow(datosNorm[datosNorm$contaminante==i & datosNorm$estacion==j, 4:35]) != 0) {
      datosNorm[datosNorm$contaminante==i & datosNorm$estacion==j, 4:35] <- scale(datosNorm[datosNorm$contaminante==i & datosNorm$estacion==j, 4:35], center = TRUE, scale = TRUE) 
    }
  }
}

# Guardamos los datos
write.csv(datosNorm, file = "datos_normalizados.csv", row.names = FALSE)

```

- Revisamos la correlación entre la media diaria de contaminación y el resto de atributos climatológicos y si el día es laboral o no

```{r correlación, message=FALSE, warning=FALSE}

j=1

for (i in levels(datosNorm$contaminante)) {
  print(paste0("Correlación de atributos para el contamiante: ", contaminante[j]))
  print(cor(datosNorm[datosNorm$contaminante==i,28:37])[,"media"])
  cat("\n")
  j=j+1
}

```

### 1.4. Caracterización del clima de Madrid

El clima de la ciudad de Madrid según la clasificación climática de Köppen es clima templado con verano seco y caluroso (Csa).

A continuación se muestran los valores climatológicos medios para cada estación del año según los datos del periodo 2013-2018.

```{r Caracterización_estaciones, message=FALSE, warning=FALSE}

# primavera (20/03 -20/06)
cat("Climatología en los meses de primavera:\n")
print(colMeans(datos[format(datos$fecha,"%m-%d")>='03-20'&format(datos$fecha,"%m-%d")<='06-20',c("prec","presMax","presMin","racha","tmax","tmed","tmin","velMedia")], na.rm = TRUE))

# verano (21/06 - 22/09)
cat("\nclimatología en los meses de verano:\n")
print(colMeans(datos[format(datos$fecha,"%m-%d")>='06-21'&format(datos$fecha,"%m-%d")<='09-22',c("prec","presMax","presMin","racha","tmax","tmed","tmin","velMedia")], na.rm = TRUE))

# otoño (23/09 - 20/12)
cat("\nClimatología en los meses de otoño:\n")
print(colMeans(datos[format(datos$fecha,"%m-%d")>='09-23'&format(datos$fecha,"%m-%d")<='12-20',c("prec","presMax","presMin","racha","tmax","tmed","tmin","velMedia")], na.rm = TRUE))

# invierno (21/12 - 19/03)
cat("\nClimatología en los meses de invierno:\n")
print(colMeans(datos[(format(datos$fecha,"%m-%d")>='12-21'&format(datos$fecha,"%m-%d")<='12-31') | (format(datos$fecha,"%m-%d")>='01-01' & format(datos$fecha,"%m-%d")<='03-19'),c("prec","presMax","presMin","racha","tmax","tmed","tmin","velMedia")], na.rm = TRUE))

```

A continuación se muestra un gráfico temporal con los datos de temperatura y precipitaciones a lo largo del año 2018:

```{r caracterización_precipitaciones, message=FALSE, warning=FALSE}

# Gráficos
library(ggplot2)

# Precipitaciones y temperatura
ggplot(data = datos, aes(fecha, prec)) + geom_line(aes(y = prec, colour = "prec")) + geom_line(aes(y = tmed, colour = "tmed")) + scale_x_date(date_breaks = "1 month", date_labels = "%m", limits=as.Date(c('2018-01-01','2018-12-31'))) + ggtitle("Precipitaciones (mm) y temperatura (ºC) en 2018")

```

Como se puede observar en el gráfico, el clima de Madrid a lo largo del año se caracteriza por una sequia estival que se corresponde con las temperaturas más elevadas.

A continuación se muestra el nivel de lluvias acumulado (mm) durante los meses del año 2018:

```{r caracterización_precipitaciones_acumulado, message=FALSE, warning=FALSE}

# Precipitaciones acumulado
ggplot(data = clima, aes(x = as.Date(cut(clima$fecha, breaks = "month")), y = prec)) + stat_summary(fun.y = cumsum, geom = "bar") + scale_x_date(date_breaks = "1 month", date_labels = "%m", limits=as.Date(c('2017-12-01','2018-12-31'))) + xlab("Fecha") + ylab("Prestaciones")

```

Durante el año 2018, el mes de mayor acumulación de lluvias fue marzo seguido de abril y mayo.

### 1.5. Modelo de predicción de la calidad del aire de Madrid

En los modelos de series temporales, los datos se recogen de manera secuencial durante el paso del tiempo.

Estos modelos son de tipo supervisado, por lo que necesitan de una variable objetivo Y y un predictor X.

Para conseguir esto, es necesario transformar la serie temporal (array de observaciones) en una nueva serie en la que se agrupen k observaciones consecutivas desde el momento t-k hasta t de forma que podamos entrenar al modelo con las k-1 observaciones anteriores a t como predictor X y la observación de t como variable objetivo Y.

Por lo tanto, para nuestro modelo elegiremos un contaminante a predecir, tomaremos sus observaciones y las almacenaremos como una serie temporal. A continuación estableceremos el número de k de observaciones que agruparemos, por ejemplo 7 días y entrenaremos el modelo con las k-1 primeras observaciones como predictor X y la observación k como variable objetivo Y.

```{r carga_datos, eval=FALSE, message=FALSE, warning=FALSE, include=FALSE}

# Carga del fichero "datos.csv" con los datos ya preprocesados

# Establecemos el nombre de las columnas
columnas <- c("estacion","fecha","contaminante","hora1","hora2","hora3","hora4","hora5","hora6","hora7","hora8","hora9","hora10","hora11","hora12","hora13","hora14","hora15","hora16","hora17","hora18","hora19","hora20","hora21","hora22","hora23","hora24","media","maximo","prec","presMax","presMin","tmax","tmed","tmin","racha","velMedia","laborable")

# Leemos el fichero "datos.csv" y almacenamos los datos en el data frame datos
datos <- read.csv(file = "datos.csv", header = TRUE, sep = ",", col.names = columnas)

# Establecemos el tipo de dato de cada columna
datos$estacion <- as.factor(datos$estacion)
datos$fecha<-as.Date(datos$fecha, format("%Y-%m-%d"))
datos$contaminante <- as.factor(datos$contaminante)

```

```{r preparar_datos, message=FALSE, warning=FALSE}

preparar_datos <- function(c, estacion, predictores) {
  
  # c: Código del contaminante seleccionado
  # estacion: Código de la estación a analizar
  # predictores: 1 sólo medidas de contaminación, 2 medidas de contaminación y climatología, 3 medidas de contaminación, climatología y calendario laboral
  
  if (is.null(c)) stop("El parámetro c (código de contaminante) es obligatorio.")
  if (is.null(estacion)) stop("El parámetro estación es obligatorio.")
  if (is.null(predictores)) stop("El parámetro preditores es obligatorio.")
  
  if (predictores == 1) {
    
    # Sólo se utilizan los datos de medidas de contaminación para el contaminante c
    
    # Almacenamos las observaciones del contaminante c como series temporales en X e Y, siendo Y[i] la siguiente medición a X[i]
    X <- datos[datos$contaminante==c&datos$estacion==estacion,"maximo"]
    
    Y <- X[-1]
    X <- X[-length(X)]
    
    # Separamos X e Y en sets de entrenamiento y test (80% y 20% respectivamente)
    sample<-c(1:(0.8*length(X)))
        
    X_train <- X[sample]
    Y_train <- Y[sample]
        
    X_test <- X[-sample]
    Y_test <- Y[-sample]
    
  } else if (predictores == 2) {
    
    # Se utilizan los datos de medidas de contaminación y las medidas climatológicas
    
    # Almacenamos las observaciones del contaminante c como series temporales en X e Y, siendo Y[i,] las siguientes mediciones a X[i,]
    X <- datos[datos$contaminante==c&datos$estacion==estacion,c("maximo","prec","presMax","presMin","tmax","tmed","tmin", "racha","velMedia")]
    
    Y <- X[-1,"maximo"]
    X <- X[-nrow(X),]
    
    # Separamos X e Y en sets de entrenamiento y test (80% y 20% respectivamente)
    sample<-c(1:(0.8*nrow(X)))
        
    X_train <- X[sample,]
    Y_train <- Y[sample]
        
    X_test <- X[-sample,]
    Y_test <- Y[-sample]
    
  } else if (predictores == 3) {
    
    # Se utilizan los datos de medidas de contaminación, las medidas climatológicas y el calendario laboral
    
    # Almacenamos las observaciones del contaminante c como series temporales en X e Y, siendo Y[i,] las siguientes mediciones a X[i,]
    X <- datos[datos$contaminante==c&datos$estacion==estacion,c("maximo","prec","presMax","presMin","tmax","tmed","tmin", "racha","velMedia","laborable")]
    
    Y <- X[-1,"maximo"]
    X <- X[-nrow(X),]
    
    # Separamos X e Y en sets de entrenamiento y test (80% y 20% respectivamente)
    sample<-c(1:(0.8*nrow(X)))
        
    X_train <- X[sample,]
    Y_train <- Y[sample]
        
    X_test <- X[-sample,]
    Y_test <- Y[-sample]
    
  }
  
  # Almacenamos los sets de datos en una lista (facilita el return de la función)
  returnList <- list("X_train" = X_train, "Y_train" = Y_train, "X_test" = X_test, "Y_test" = Y_test)
  
  # Normalizamos los datos
  returnList <- normalizar(returnList)
  
  return(returnList)
  
}

normalizar <- function(dataList) {
  
  # Normalizamos los datos entre 0 y 1
  
  if (typeof(dataList$X_train) == "list") {
    # X_train y X_test contienen varios predictores
    
    # Calculamos los máximos y mínimos de cada predictor
    dataList$maxXTrain = sapply(dataList$X_train, max)
    dataList$minXTrain = sapply(dataList$X_train, min)
    dataList$maxXTest = sapply(dataList$X_test, max)
    dataList$minXTest = sapply(dataList$X_test, min)
    
    # Normalizamos cada columna
    for (i in 1:length(dataList$X_train)) {
      
      dataList$X_train[,i] <- (dataList$X_train[,i] - dataList$minXTrain[i]) / (dataList$maxXTrain[i] - dataList$minXTrain[i])
      dataList$X_test[,i] <- (dataList$X_test[,i] - dataList$minXTest[i]) / (dataList$maxXTest[i] - dataList$minXTest[i])
      
    }
    
  } else {
    
    # X_train y X_test contienen un único predictor
    
    # Calculamos el máximo y el mínimo del predictor
    dataList$maxXTrain = max(dataList$X_train)
    dataList$minXTrain = min(dataList$X_train)
    dataList$maxXTest = max(dataList$X_test)
    dataList$minXTest = min(dataList$X_test)
    
    # Normalizamos los datos
    dataList$X_train <- (dataList$X_train - dataList$minXTrain) / (dataList$maxXTrain - dataList$minXTrain)
    dataList$X_test <- (dataList$X_test - dataList$minXTest) / (dataList$maxXTest - dataList$minXTest)
    
  }

  # Calculamos el máximo y el mínimo de la variable objetivo
  dataList$maxYTrain = max(dataList$Y_train)
  dataList$minYTrain = min(dataList$Y_train)
  dataList$maxYTest = max(dataList$Y_test)
  dataList$minYTest = min(dataList$Y_test)
  
  # Normalizamos los datos
  dataList$Y_train <- (dataList$Y_train - dataList$minYTrain) / (dataList$maxYTrain - dataList$minYTrain)
  dataList$Y_test <- (dataList$Y_test - dataList$minYTest) / (dataList$maxYTest - dataList$minYTest)
  
  return(dataList)
  
}

```

```{r crear_modelo_mlp, message=FALSE, warning=FALSE}

crear_modelo_mlp <- function(batch_size, n, units) {
  
  # Importamos la librería keras para crear el modelo
  library(keras)
  
  # Destroys the current TF graph and creates a new one. Useful to avoid clutter from old models / layers.
  k_clear_session()
  
  # Creamos un modelo secuencial como una pila lineal de capas
  modelo <- keras_model_sequential()
  
  # Añadimos las capas al modelo
  modelo %>%
    layer_dense(units = units, batch_input_shape = c(batch_size, n)) %>% 
    layer_dense(units = units) %>% 
    layer_dense(units = units) %>% 
    layer_dense(units = 1)
  
  # Compilamos el modelo
  modelo %>% compile(loss = 'mean_squared_error', optimizer = 'adam')
  
  return(modelo)

}

```

```{r crear_modelo_lstm, message=FALSE, warning=FALSE}

crear_modelo_lstm <- function(batch_size, n, units) {
  
  # Importamos la librería keras para crear el modelo
  library(keras)
  
  # Destroys the current TF graph and creates a new one. Useful to avoid clutter from old models / layers.
  k_clear_session()
  
  # Creamos un modelo secuencial como una pila lineal de capas
  modelo <- keras_model_sequential()
  
  # Añadimos las capas al modelo
  modelo %>%
    layer_lstm(units = units, batch_input_shape = c(batch_size, 1, n), 
               return_sequences = TRUE, stateful=TRUE) %>% 
    layer_lstm(units = units, return_sequences = TRUE, stateful=TRUE) %>% 
    layer_lstm(units = units, return_sequences = FALSE, stateful=TRUE) %>% 
    layer_dense(units = 1)
  
  # Compilamos el modelo
  modelo %>% compile(loss = 'mean_squared_error', optimizer = 'RMSprop')
  
  return(modelo)

}

```

```{r entrenar_modelo_mlp, message=FALSE, warning=FALSE}

entrenar_modelo_mlp <- function(modelo, batch_size, epochs) {
  
  if(typeof(X_train)=="list") {
    # X_train contiene varios predictores
    
    # Truncamos las observaciones de X_train e Y_train para coincidir con el batch_size
    num = batch_size*as.integer(nrow(X_train)/batch_size)
  
    # Convertimos el data frame X_train en una matriz
    X_train <- data.matrix(X_train[1:num,]) #Si X_train contiene varias columnas
  
  } else {
    # X_train contiene un único predictor
  
    # Truncamos las observaciones de X_train e Y_train para coincidir con el batch_size
    num = batch_size*as.integer(length(X_train)/batch_size)
  
    # Convertimos el data frame X_train en una matriz
    X_train <- data.matrix(X_train[1:num]) #Si X_train es un Array

  }
  
  # Entrenamos el modelo
  modelo %>% fit(X_train, Y_train[1:num], batch_size = batch_size, epochs = epochs, verbose = 1, shuffle = FALSE)
  
}

```

```{r entrenar_modelo_lstm, message=FALSE, warning=FALSE}

entrenar_modelo_lstm <- function(modelo, batch_size, epochs) {
  
  if(typeof(X_train)=="list") {
    # X_train contiene varios predictores
    
    # Truncamos las observaciones de X_train e Y_train para coincidir con el batch_size
    num = batch_size*as.integer(nrow(X_train)/batch_size)
  
    # Convertimos el data frame X_train en una matriz
    X_train <- data.matrix(X_train[1:num,]) #Si X_train contiene varias columnas
  
    # Redimensionamos la matrix X_train
    dim(X_train) <- c(dim(X_train)[1],1,dim(X_train)[2])
  
  } else {
    # X_train contiene un único predictor
  
    # Truncamos las observaciones de X_train e Y_train para coincidir con el batch_size
    num = batch_size*as.integer(length(X_train)/batch_size)
  
    # Convertimos el data frame X_train en una matriz
    X_train <- data.matrix(X_train[1:num]) #Si X_train es un Array
  
    # Redimensionamos la matrix X_train
    dim(X_train) <- c(dim(X_train)[1],1,dim(X_train)[2])

  }
  
  # Entrenamos el modelo
  modelo %>% fit(X_train, Y_train[1:num], batch_size = batch_size, epochs = epochs, verbose = 1, shuffle = FALSE)
  
}

```

```{r predicciones_modelo_mlp, message=FALSE, warning=FALSE}

predicciones_modelo_mlp <- function(modelo, batch_size) {
  
  if(typeof(X_test)=="list") {
    # X_test contiene varios predictores
    
    # Truncamos las observaciones de X_test e Y_test para coincidir con el batch_size
    num = as.integer(nrow(X_test)/batch_size)*batch_size
  
    # Convertimos el data frame X_train en una matriz
    X_test <- data.matrix(X_test[1:num,]) #Si X_test contiene varias columnas
  
  } else {
    # X_test contiene un único predictor
  
    # Truncamos las observaciones de X_test e Y_test para coincidir con el batch_size
    num = as.integer(length(X_test)/batch_size)*batch_size
  
    # Convertimos el data frame X_test en una matriz
    X_test <- data.matrix(X_test[1:num]) #Si X_test es un Array
  
  }
  
  # Usamos el modelo para realizar las predicciones
  Y_pred <- modelo %>% predict(X_test, batch_size = batch_size)
  
  return(Y_pred)
  
}

```

```{r predicciones_modelo_lstm, message=FALSE, warning=FALSE}

predicciones_modelo_lstm <- function(modelo, batch_size) {
  
  if(typeof(X_test)=="list") {
    # X_test contiene varios predictores
    
    # Truncamos las observaciones de X_test e Y_test para coincidir con el batch_size
    num = as.integer(nrow(X_test)/batch_size)*batch_size
  
    # Convertimos el data frame X_train en una matriz
    X_test <- data.matrix(X_test[1:num,]) #Si X_test contiene varias columnas
  
    # Redimensionamos la matrix X_train
    dim(X_test) <- c(dim(X_test)[1],1,dim(X_test)[2])
  
  } else {
    # X_test contiene un único predictor
  
    # Truncamos las observaciones de X_test e Y_test para coincidir con el batch_size
    num = as.integer(length(X_test)/batch_size)*batch_size
  
    # Convertimos el data frame X_test en una matriz
    X_test <- data.matrix(X_test[1:num]) #Si X_test es un Array
  
    # Redimensionamos la matrix X_test
    dim(X_test) <- c(dim(X_test)[1],1,dim(X_test)[2])
  
  }
  
  # Usamos el modelo para realizar las predicciones
  Y_pred <- modelo %>% predict(X_test, batch_size = batch_size)
  
  return(Y_pred)
  
}

```

```{r evaluar_modelo}

evaluar_modelo <- function() {
  
  # Evaluamos el modelo calculando la raíz cuadrada del error cuadrático medio (RMSE)
  return(sqrt(mean((Y_pred - Y_test[1:length(Y_pred)])^2)))
  
}

```

```{r mostrar_prediccion, message=FALSE, warning=FALSE}

comparar_prediccion<-function(Y_test,Y_pred) {
  
  # Importamos la librería ggplot2
  library(ggplot2)
  
  # Creamos un data frame con los datos reales y las predicciones
  resultado <- cbind(Y_test[1:length(Y_pred)],Y_pred)
  resultado <- as.data.frame(resultado)
  
  # Creamos un vector X con para el eje de abscisas
  X <- c(1:length(Y_pred))
  
  # Mostramos los datos reales y las predicciones con un gráfico de líneas
  p <- ggplot() + geom_line(data = resultado, aes(x=X, y=resultado$V1, colour = "Y_test"), size = 1) + geom_line(data = resultado, aes(x=X, y=resultado$V2, colour = "Y_pred"), size = 1) + xlab("Tiempo") + ylab("Contaminación")
  
  print(p)
  
}

```

```{r benchmark_modelo_mlp, eval=FALSE, message=FALSE, warning=FALSE, include=FALSE}

c = 8 # Dióxido de nitrógeno
estacion = 28079004 # Plaza de España
predictores = c(1,2,3) # Numero de variables predictoras a considerar
batch_size_valores = c(1,7,30) # batch_sizes a evaluar
units_valores = c(1,8,16,32,64) # Numero de units a evaluar
epochs_valores = c(1,5,10) # Número de épocas a evaluar

resultados <- data.frame()

for (k in predictores) {
    
  # n contiene el numero de predictores en cada observacion (n=1 si k=1, n=9 si k=2, n=10 si k=3)
  n = switch(k,1,9,10)
  
  for (batch_size in batch_size_valores) {
    for (units in units_valores) {
      for (epochs in epochs_valores) {
        
        cat("Evaluando modelo MLP con c:",c,"predictores:",k,"batch_size:",batch_size,"units:",units,"epochs:",epochs,"\n")
        
        # Preparamos los datos de entrenamiento y prueba
        returnList <- preparar_datos(c, estacion, k)
        
        X_train <- returnList$X_train
        Y_train <- returnList$Y_train
        X_test <- returnList$X_test
        Y_test <- returnList$Y_test
        
        # Creamos el modelo MLP  
        modelo <- crear_modelo_mlp(batch_size, n, units)
        
        # Entrenamos el modelo con X_train e Y_train (variables globales)
        entrenar_modelo_mlp(modelo = modelo, batch_size, epochs)
        
        # Predecimos los resultados para X_test (variable global)
        Y_pred <- predicciones_modelo_mlp(modelo = modelo, batch_size)
        
        # Evaluamos el modelo calculando el RMSE
        rmse = evaluar_modelo()
        
        # Revertimos la normalización de los datos de test
        Y_pred <- (Y_pred * (returnList$maxYTest - returnList$minYTest)) + returnList$minYTest
        Y_test <- (Y_test * (returnList$maxYTest - returnList$minYTest)) + returnList$minYTest
        comparar_prediccion(Y_test,Y_pred)
        
        # Almacenamos los resultados de los hiperparámetros evaluados en el fichero "mlp_resultados_bench.csv"
        resultados <- rbind(resultados, c(c,k,batch_size,units,epochs,rmse))
        colnames(resultados) <- c("c","k","batch_size","units","epochs","rmse")
        write.csv2(resultados, file = "mlp_resultados_bench.csv", row.names= FALSE, col.names = TRUE)
        
      }
    }
  }
}

```

```{r benchmark_modelo_lstm, eval=FALSE, message=FALSE, warning=FALSE, include=FALSE}

c = 8 # Dióxido de nitrógeno
estacion = 28079004 # Plaza de España
predictores = c(1,2,3) # Numero de variables predictoras a considerar
batch_size_valores = c(1,7,30) # batch_sizes a evaluar
units_valores = c(1,8,16,32,64) # Numero de units a evaluar
epochs_valores = c(1,5,10) # Número de épocas a evaluar

resultados <- data.frame()

for (k in predictores) {
    
  # n contiene el numero de predictores en cada observacion (n=1 si k=1, n=9 si k=2, n=10 si k=3)
  n = switch(k,1,9,10)
  
  for (batch_size in batch_size_valores) {
    for (units in units_valores) {
      for (epochs in epochs_valores) {
        
        cat("Evaluando modelo LSTM con c:",c,"predictores:",k,"batch_size:",batch_size,"units:",units,"epochs:",epochs,"\n")
        
        # Preparamos los datos de entrenamiento y prueba
        returnList <- preparar_datos(c, estacion, k)
        
        X_train <- returnList$X_train
        Y_train <- returnList$Y_train
        X_test <- returnList$X_test
        Y_test <- returnList$Y_test
        
        # Creamos el modelo LSTM  
        modelo <- crear_modelo_lstm(batch_size, n, units)
        
        # Entrenamos el modelo con X_train e Y_train (variables globales)
        entrenar_modelo_lstm(modelo = modelo, batch_size, epochs)
        
        # Predecimos los resultados para X_test (variable global)
        Y_pred <- predicciones_modelo_lstm(modelo = modelo, batch_size)
        
        # Evaluamos el modelo calculando el RMSE
        rmse = evaluar_modelo()
        
        # Revertimos la normalización de los datos de test
        Y_pred <- (Y_pred * (returnList$maxYTest - returnList$minYTest)) + returnList$minYTest
        Y_test <- (Y_test * (returnList$maxYTest - returnList$minYTest)) + returnList$minYTest
        comparar_prediccion(Y_test,Y_pred)
        
        # Almacenamos los resultados de los hiperparámetros evaluados en el fichero "lstm_resultados_bench.csv"
        resultados <- rbind(resultados, c(c,k,batch_size,units,epochs,rmse))
        colnames(resultados) <- c("c","k","batch_size","units","epochs","rmse")
        write.csv2(resultados, file = "lstm_resultados_bench.csv", row.names= FALSE, col.names = TRUE)
        
      }
    }
  }
}

```

```{r modelo_mlp, eval=FALSE, message=FALSE, warning=FALSE, include=FALSE}

# Establecemos los hiperparámetros del modelo LSTM
cod_contaminante = 8 # Dióxido de nitrógeno
cod_estacion = 28079004 # Plaza de España
predictores = 3 # Como variables predictoras utilizamos las medidas de contaminación, las medidas climatológicas y el calendario laboral
n = switch(predictores,1,9,10) # n contiene el numero de predictores en cada observacion (n=1 si k=1, n=9 si k=2, n=10 si k=3)
batch_size = 7
units = 128
epochs = 10
        
cat("Predicción mediante modelo MLP.\n Contaminante:",cod_contaminante,"predictores:",predictores,"batch_size:",batch_size,"units:",units,"epochs:",epochs,"\n")

# Preparamos los datos de entrenamiento y prueba
returnList <- preparar_datos(cod_contaminante, cod_estacion, predictores)

X_train <- returnList$X_train
Y_train <- returnList$Y_train
X_test <- returnList$X_test
Y_test <- returnList$Y_test

# Creamos el modelo MLP  
modelo <- crear_modelo_mlp(batch_size, n, units)

# Entrenamos el modelo con X_train e Y_train (variables globales)
entrenar_modelo_mlp(modelo = modelo, batch_size, epochs)

# Predecimos los resultados para X_test (variable global)
Y_pred <- predicciones_modelo_mlp(modelo = modelo, batch_size)

# Evaluamos el modelo calculando el RMSE
rmse = evaluar_modelo()

# Revertimos la normalización de los datos de test
Y_pred <- (Y_pred * (returnList$maxYTest - returnList$minYTest)) + returnList$minYTest
Y_test <- (Y_test * (returnList$maxYTest - returnList$minYTest)) + returnList$minYTest

# Comparamos los datos reales y las predicciones con un gráfico de líneas 
comparar_prediccion(Y_test,Y_pred)

```

```{r modelo_lstm, eval=FALSE, message=FALSE, warning=FALSE, include=FALSE}

# Establecemos los hiperparámetros del modelo LSTM
cod_contaminante = 8 # Dióxido de nitrógeno
cod_estacion = 28079004 # Plaza de España
predictores = 3 # Como variables predictoras utilizamos las medidas de contaminación, las medidas climatológicas y el calendario laboral
n = switch(predictores,1,9,10) # n contiene el numero de predictores en cada observacion (n=1 si k=1, n=9 si k=2, n=10 si k=3)
batch_size = 30
units = 32
epochs = 10
        
cat("Predicción mediante modelo LSTM.\n Contaminante:",cod_contaminante,"predictores:",predictores,"batch_size:",batch_size,"units:",units,"epochs:",epochs,"\n")

# Preparamos los datos de entrenamiento y prueba
returnList <- preparar_datos(cod_contaminante, cod_estacion, predictores)

X_train <- returnList$X_train
Y_train <- returnList$Y_train
X_test <- returnList$X_test
Y_test <- returnList$Y_test

# Creamos el modelo LSTM  
modelo <- crear_modelo_lstm(batch_size, n, units)

# Entrenamos el modelo con X_train e Y_train (variables globales)
entrenar_modelo_lstm(modelo = modelo, batch_size, epochs)

# Predecimos los resultados para X_test (variable global)
Y_pred <- predicciones_modelo_lstm(modelo = modelo, batch_size)

# Evaluamos el modelo calculando el RMSE
rmse = evaluar_modelo()

# Revertimos la normalización de los datos de test
Y_pred <- (Y_pred * (returnList$maxYTest - returnList$minYTest)) + returnList$minYTest
Y_test <- (Y_test * (returnList$maxYTest - returnList$minYTest)) + returnList$minYTest

# Comparamos los datos reales y las predicciones con un gráfico de líneas 
comparar_prediccion(Y_test,Y_pred)

```